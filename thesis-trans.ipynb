{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:03.118917Z",
     "iopub.status.busy": "2026-01-05T19:38:03.118630Z",
     "iopub.status.idle": "2026-01-05T19:38:06.903731Z",
     "shell.execute_reply": "2026-01-05T19:38:06.902964Z",
     "shell.execute_reply.started": "2026-01-05T19:38:03.118893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "DATA_ROOT OK: /kaggle/input/sen2fire/Sen2Fire/Sen2Fire\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 24\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    DATA_ROOT: str = \"/kaggle/input/sen2fire/Sen2Fire/Sen2Fire\"\n",
    "    SCENES: Tuple[str, ...] = (\"scene1\", \"scene2\", \"scene3\", \"scene4\")\n",
    "    PATCH_EXT: str = \".npz\"\n",
    "\n",
    "    # Global split (VD default)\n",
    "    USE_GLOBAL_SPLIT: bool = True\n",
    "    GLOBAL_TRAIN_RATIO: float = 0.80\n",
    "    GLOBAL_VAL_RATIO: float = 0.10\n",
    "    GLOBAL_TEST_RATIO: float = 0.10\n",
    "    KEEP_VAL_TEST_NATURAL: bool = True\n",
    "\n",
    "    # Keys inside npz\n",
    "    X_KEY: str = \"image\"     # (12,512,512)\n",
    "    A_KEY: str = \"aerosol\"   # (512,512) -> becomes 1 channel\n",
    "    Y_KEY: str = \"label\"     # (512,512)\n",
    "\n",
    "    # Fire patch definition\n",
    "    FIRE_PATCH_MIN_RATIO: float = 0\n",
    "\n",
    "    # Controlled train pool (VD default)\n",
    "    USE_CONTROLLED_POOL: bool = True\n",
    "    POOL_KEEP_FIRE: int = -1\n",
    "    NONFIRE_PER_FIRE: int = 3\n",
    "\n",
    "    # Training config (RAM-safe)\n",
    "    IN_CHANNELS: int = 13\n",
    "    H: int = 512\n",
    "    W: int = 512\n",
    "    BATCH_SIZE: int = 2\n",
    "    NUM_WORKERS: int = 2\n",
    "    LR: float = 1e-4\n",
    "    EPOCHS: int = 10\n",
    "\n",
    "    VERBOSE: bool = True\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "assert abs((cfg.GLOBAL_TRAIN_RATIO + cfg.GLOBAL_VAL_RATIO + cfg.GLOBAL_TEST_RATIO) - 1.0) < 1e-6\n",
    "assert os.path.exists(cfg.DATA_ROOT), f\"DATA_ROOT not found: {cfg.DATA_ROOT}\"\n",
    "print(\"DATA_ROOT OK:\", cfg.DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Risk Mapping (Sen2Fire) — Notebook 2 (DeepLabV3+)\n",
    "\n",
    "Same pipeline as Notebook 1 (UNet++):\n",
    "- Global stratified split + controlled train pool\n",
    "- Normalization (13-channel)\n",
    "- Segmentation training\n",
    "- t_high selection by validation mean Dice sweep\n",
    "- Export Streamlit-ready model package\n",
    "\n",
    "Export folder:\n",
    "- /kaggle/working/model_packages/deeplabv3p_fire/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine 1 — Patch Intake (NPZ → X, Y)\n",
    "\n",
    "Inside each patch (.npz):\n",
    "- `image`: (12,512,512) Sentinel-2 bands\n",
    "- `aerosol`: (512,512) additional band\n",
    "- `label`: (512,512) fire mask (0/1)\n",
    "\n",
    "We construct:\n",
    "- X: concat(image[12], aerosol[1]) → (13,512,512)\n",
    "- Y: label → (1,512,512)\n",
    "\n",
    "Note:\n",
    "- Y is only used during training/validation to compute loss/metrics.\n",
    "- During Streamlit inference, we only have X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:08.047878Z",
     "iopub.status.busy": "2026-01-05T19:38:08.047468Z",
     "iopub.status.idle": "2026-01-05T19:38:08.117596Z",
     "shell.execute_reply": "2026-01-05T19:38:08.117046Z",
     "shell.execute_reply.started": "2026-01-05T19:38:08.047851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene1: 864 patches\n",
      "scene2: 594 patches\n",
      "scene3: 504 patches\n",
      "scene4: 504 patches\n",
      "Total patches: 2466\n"
     ]
    }
   ],
   "source": [
    "def list_scene_files(data_root: str, scene_name: str, ext: str = \".npz\") -> List[str]:\n",
    "    scene_dir = os.path.join(data_root, scene_name)\n",
    "    return sorted(glob.glob(os.path.join(scene_dir, f\"*{ext}\")))\n",
    "\n",
    "scene_to_files = {}\n",
    "total = 0\n",
    "for s in cfg.SCENES:\n",
    "    files = list_scene_files(cfg.DATA_ROOT, s, cfg.PATCH_EXT)\n",
    "    scene_to_files[s] = files\n",
    "    total += len(files)\n",
    "    print(f\"{s}: {len(files)} patches\")\n",
    "\n",
    "print(\"Total patches:\", total)\n",
    "if total == 0:\n",
    "    raise RuntimeError(\"No patch files found. Check DATA_ROOT / PATCH_EXT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:10.755516Z",
     "iopub.status.busy": "2026-01-05T19:38:10.754894Z",
     "iopub.status.idle": "2026-01-05T19:38:10.877778Z",
     "shell.execute_reply": "2026-01-05T19:38:10.877027Z",
     "shell.execute_reply.started": "2026-01-05T19:38:10.755486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: /kaggle/input/sen2fire/Sen2Fire/Sen2Fire/scene1/scene_1_patch_10_1.npz\n",
      "  -      image: shape=(12, 512, 512), dtype=int16\n",
      "  -    aerosol: shape=(512, 512), dtype=float32\n",
      "  -      label: shape=(512, 512), dtype=uint8\n"
     ]
    }
   ],
   "source": [
    "def inspect_npz(npz_path: str):\n",
    "    with np.load(npz_path) as data:\n",
    "        return {k: (data[k].shape, str(data[k].dtype)) for k in data.keys()}\n",
    "\n",
    "sample_path = scene_to_files[cfg.SCENES[0]][0]\n",
    "print(\"Sample file:\", sample_path)\n",
    "info = inspect_npz(sample_path)\n",
    "for k, (shape, dtype) in info.items():\n",
    "    print(f\"  - {k:>10s}: shape={shape}, dtype={dtype}\")\n",
    "\n",
    "for rk in [cfg.X_KEY, cfg.A_KEY, cfg.Y_KEY]:\n",
    "    if rk not in info:\n",
    "        raise KeyError(f\"Missing key '{rk}' in npz. Found keys: {list(info.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:15.510363Z",
     "iopub.status.busy": "2026-01-05T19:38:15.509621Z",
     "iopub.status.idle": "2026-01-05T19:38:46.478095Z",
     "shell.execute_reply": "2026-01-05T19:38:46.477379Z",
     "shell.execute_reply.started": "2026-01-05T19:38:15.510332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FULL dataset has_fire distribution:\n",
      "has_fire\n",
      "0    2117\n",
      "1     349\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FULL dataset has_fire ratio:\n",
      "has_fire\n",
      "0    0.858475\n",
      "1    0.141525\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for s, files in scene_to_files.items():\n",
    "    for p in files:\n",
    "        rows.append({\"scene\": s, \"path\": p})\n",
    "manifest = pd.DataFrame(rows)\n",
    "\n",
    "def fire_ratio_from_path(npz_path: str) -> float:\n",
    "    with np.load(npz_path) as data:\n",
    "        y = data[cfg.Y_KEY]\n",
    "        if y.ndim == 3 and y.shape[-1] == 1:\n",
    "            y = y[..., 0]\n",
    "        yb = (y > 0).astype(np.uint8)\n",
    "        return float(yb.mean())\n",
    "\n",
    "fire_ratios = []\n",
    "has_fire = []\n",
    "for p in manifest[\"path\"].tolist():\n",
    "    r = fire_ratio_from_path(p)\n",
    "    fire_ratios.append(r)\n",
    "    has_fire.append(1 if r > cfg.FIRE_PATCH_MIN_RATIO else 0)\n",
    "\n",
    "manifest[\"fire_ratio\"] = fire_ratios\n",
    "manifest[\"has_fire\"] = has_fire\n",
    "\n",
    "print(\"\\nFULL dataset has_fire distribution:\")\n",
    "print(manifest[\"has_fire\"].value_counts().sort_index())\n",
    "print(\"\\nFULL dataset has_fire ratio:\")\n",
    "print(manifest[\"has_fire\"].value_counts(normalize=True).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:46.479708Z",
     "iopub.status.busy": "2026-01-05T19:38:46.479409Z",
     "iopub.status.idle": "2026-01-05T19:38:46.503631Z",
     "shell.execute_reply": "2026-01-05T19:38:46.503053Z",
     "shell.execute_reply.started": "2026-01-05T19:38:46.479683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GLOBAL split counts:\n",
      "  Train pool: 1973\n",
      "  Val      : 247\n",
      "  Test     : 246\n",
      "\n",
      "TRAIN controlled sampling:\n",
      "  fire_total_in_pool   : 279\n",
      "  nofire_total_in_pool : 1694\n",
      "  fire_kept            : 279\n",
      "  nofire_kept          : 837 (NONFIRE_PER_FIRE=3)\n",
      "  train_final          : 1116\n",
      "\n",
      "Final used split sizes:\n",
      "  train: 1116\n",
      "  val  : 247\n",
      "  test : 246\n"
     ]
    }
   ],
   "source": [
    "def stratified_split(df, train_ratio, val_ratio, seed=42):\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    parts = []\n",
    "    for cls in [0, 1]:\n",
    "        sub = df[df[\"has_fire\"] == cls].copy()\n",
    "        n = len(sub)\n",
    "        n_train = int(round(n * train_ratio))\n",
    "        n_val   = int(round(n * val_ratio))\n",
    "        sub_train = sub.iloc[:n_train]\n",
    "        sub_val   = sub.iloc[n_train:n_train+n_val]\n",
    "        sub_test  = sub.iloc[n_train+n_val:]\n",
    "        parts.append((sub_train, sub_val, sub_test))\n",
    "\n",
    "    train_df = pd.concat([parts[0][0], parts[1][0]]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    val_df   = pd.concat([parts[0][1], parts[1][1]]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    test_df  = pd.concat([parts[0][2], parts[1][2]]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_pool_df, val_df, test_df = stratified_split(\n",
    "    manifest,\n",
    "    train_ratio=cfg.GLOBAL_TRAIN_RATIO,\n",
    "    val_ratio=cfg.GLOBAL_VAL_RATIO,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(\"\\nGLOBAL split counts:\")\n",
    "print(\"  Train pool:\", len(train_pool_df))\n",
    "print(\"  Val      :\", len(val_df))\n",
    "print(\"  Test     :\", len(test_df))\n",
    "\n",
    "# Controlled pool applies to TRAIN only\n",
    "if cfg.USE_CONTROLLED_POOL:\n",
    "    fire_df   = train_pool_df[train_pool_df[\"has_fire\"] == 1].copy()\n",
    "    nofire_df = train_pool_df[train_pool_df[\"has_fire\"] == 0].copy()\n",
    "\n",
    "    n_fire_total = len(fire_df)\n",
    "    n_nofire_total = len(nofire_df)\n",
    "\n",
    "    n_fire_keep = n_fire_total if cfg.POOL_KEEP_FIRE in (-1, None) else min(cfg.POOL_KEEP_FIRE, n_fire_total)\n",
    "    fire_keep = fire_df.sample(n=n_fire_keep, random_state=SEED) if n_fire_keep > 0 else fire_df.iloc[:0]\n",
    "\n",
    "    n_nofire_keep = min(n_nofire_total, n_fire_keep * int(cfg.NONFIRE_PER_FIRE))\n",
    "    nofire_keep = nofire_df.sample(n=n_nofire_keep, random_state=SEED) if n_nofire_keep > 0 else nofire_df.iloc[:0]\n",
    "\n",
    "    train_df = pd.concat([fire_keep, nofire_keep]).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nTRAIN controlled sampling:\")\n",
    "    print(f\"  fire_total_in_pool   : {n_fire_total}\")\n",
    "    print(f\"  nofire_total_in_pool : {n_nofire_total}\")\n",
    "    print(f\"  fire_kept            : {len(fire_keep)}\")\n",
    "    print(f\"  nofire_kept          : {len(nofire_keep)} (NONFIRE_PER_FIRE={cfg.NONFIRE_PER_FIRE})\")\n",
    "    print(f\"  train_final          : {len(train_df)}\")\n",
    "else:\n",
    "    train_df = train_pool_df.copy()\n",
    "\n",
    "train_paths = train_df[\"path\"].tolist()\n",
    "val_paths   = val_df[\"path\"].tolist()\n",
    "test_paths  = test_df[\"path\"].tolist()\n",
    "\n",
    "# leakage check\n",
    "assert len(set(train_paths)&set(val_paths))==0\n",
    "assert len(set(train_paths)&set(test_paths))==0\n",
    "assert len(set(val_paths)&set(test_paths))==0\n",
    "\n",
    "print(\"\\nFinal used split sizes:\")\n",
    "print(\"  train:\", len(train_paths))\n",
    "print(\"  val  :\", len(val_paths))\n",
    "print(\"  test :\", len(test_paths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine 2 — Normalization (X only)\n",
    "\n",
    "We compute mean/std per band (13 values each) from TRAIN ONLY:\n",
    "X_norm[c] = (X[c] - mean[c]) / std[c]\n",
    "\n",
    "Mini example:\n",
    "- raw pixel = 820\n",
    "- mean = 600\n",
    "- std = 100\n",
    "- norm = 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:46.504714Z",
     "iopub.status.busy": "2026-01-05T19:38:46.504521Z",
     "iopub.status.idle": "2026-01-05T19:38:46.511138Z",
     "shell.execute_reply": "2026-01-05T19:38:46.510394Z",
     "shell.execute_reply.started": "2026-01-05T19:38:46.504696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Sen2FireDataset(Dataset):\n",
    "    def __init__(self, paths: List[str], with_label: bool = True):\n",
    "        self.paths = paths\n",
    "        self.with_label = with_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        with np.load(p) as d:\n",
    "            img12 = d[cfg.X_KEY].astype(np.float32)      # (12,512,512)\n",
    "            aer   = d[cfg.A_KEY].astype(np.float32)[None, ...]  # (1,512,512)\n",
    "            x = np.concatenate([img12, aer], axis=0)     # (13,512,512)\n",
    "\n",
    "            if self.with_label:\n",
    "                y = d[cfg.Y_KEY]\n",
    "                if y.ndim == 2:\n",
    "                    y = y[None, ...]\n",
    "                y = (y > 0).astype(np.float32)\n",
    "                return torch.from_numpy(x), torch.from_numpy(y)\n",
    "            else:\n",
    "                return torch.from_numpy(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:46.512741Z",
     "iopub.status.busy": "2026-01-05T19:38:46.512511Z",
     "iopub.status.idle": "2026-01-05T19:38:46.527898Z",
     "shell.execute_reply": "2026-01-05T19:38:46.527231Z",
     "shell.execute_reply.started": "2026-01-05T19:38:46.512712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Sen2FireDataset(train_paths, with_label=True),\n",
    "                          batch_size=cfg.BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(Sen2FireDataset(val_paths, with_label=True),\n",
    "                        batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(Sen2FireDataset(test_paths, with_label=True),\n",
    "                         batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=cfg.NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:46.528857Z",
     "iopub.status.busy": "2026-01-05T19:38:46.528645Z",
     "iopub.status.idle": "2026-01-05T19:38:50.495810Z",
     "shell.execute_reply": "2026-01-05T19:38:50.495103Z",
     "shell.execute_reply.started": "2026-01-05T19:38:46.528838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN_13 len: 13 STD_13 len: 13\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def compute_mean_std(loader, max_batches=50):\n",
    "    mean = torch.zeros(cfg.IN_CHANNELS, device=DEVICE)\n",
    "    var  = torch.zeros(cfg.IN_CHANNELS, device=DEVICE)\n",
    "    n_batches = 0\n",
    "\n",
    "    for bi, (x, y) in enumerate(loader):\n",
    "        if bi >= max_batches:\n",
    "            break\n",
    "        x = x.to(DEVICE)  # (B,C,H,W)\n",
    "        x_ = x.view(x.size(0), cfg.IN_CHANNELS, -1)\n",
    "        mean += x_.mean(dim=(0,2))\n",
    "        var  += x_.var(dim=(0,2), unbiased=False)\n",
    "        n_batches += 1\n",
    "\n",
    "    mean /= max(n_batches, 1)\n",
    "    var  /= max(n_batches, 1)\n",
    "    std = torch.sqrt(var + 1e-6)\n",
    "    return mean.detach().cpu().tolist(), std.detach().cpu().tolist()\n",
    "\n",
    "MEAN_13, STD_13 = compute_mean_std(train_loader, max_batches=50)\n",
    "print(\"MEAN_13 len:\", len(MEAN_13), \"STD_13 len:\", len(STD_13))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:50.497237Z",
     "iopub.status.busy": "2026-01-05T19:38:50.496968Z",
     "iopub.status.idle": "2026-01-05T19:38:54.795814Z",
     "shell.execute_reply": "2026-01-05T19:38:54.795144Z",
     "shell.execute_reply.started": "2026-01-05T19:38:50.497210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T19:38:54.797309Z",
     "iopub.status.busy": "2026-01-05T19:38:54.797039Z",
     "iopub.status.idle": "2026-01-05T19:38:54.802349Z",
     "shell.execute_reply": "2026-01-05T19:38:54.801605Z",
     "shell.execute_reply.started": "2026-01-05T19:38:54.797280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def normalize_batch(x, mean_13, std_13):\n",
    "    mean_t = torch.tensor(mean_13, device=x.device).view(1, cfg.IN_CHANNELS, 1, 1)\n",
    "    std_t  = torch.tensor(std_13,  device=x.device).view(1, cfg.IN_CHANNELS, 1, 1)\n",
    "    return (x - mean_t) / (std_t + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TES MODEL: TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T12:47:23.937379Z",
     "iopub.status.busy": "2026-01-04T12:47:23.936947Z",
     "iopub.status.idle": "2026-01-04T12:47:25.647881Z",
     "shell.execute_reply": "2026-01-04T12:47:25.647153Z",
     "shell.execute_reply.started": "2026-01-04T12:47:23.937342Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b05f994021a45489b3bc027ebf118b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d099fa3d21ff471ea52e0f9a9ec4954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/54.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b1-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegformerForSemanticSegmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b234dbb29d75414683d382bbd261fc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/54.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "def build_segformer(in_channels=13, num_classes=1, backbone=\"nvidia/segformer-b1-finetuned-ade-512-512\"): #MiT-B2\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        backbone,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    # ganti input channel (default 3 → 13)\n",
    "    model.segformer.encoder.patch_embeddings[0].proj = \\\n",
    "        torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            model.segformer.encoder.patch_embeddings[0].proj.out_channels,\n",
    "            kernel_size=model.segformer.encoder.patch_embeddings[0].proj.kernel_size,\n",
    "            stride=model.segformer.encoder.patch_embeddings[0].proj.stride,\n",
    "            padding=model.segformer.encoder.patch_embeddings[0].proj.padding,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    return model\n",
    "    \n",
    "model = build_segformer(cfg.IN_CHANNELS).to(DEVICE)\n",
    "print(type(model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T12:57:32.193400Z",
     "iopub.status.busy": "2026-01-04T12:57:32.192569Z",
     "iopub.status.idle": "2026-01-04T12:57:32.216769Z",
     "shell.execute_reply": "2026-01-04T12:57:32.215915Z",
     "shell.execute_reply.started": "2026-01-04T12:57:32.193348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.LR)\n",
    "scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "def train_one_epoch(model, loader, thr=0.5, eps=1e-6):\n",
    "    model.train()\n",
    "    total_loss_sum = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    total_tp = total_fp = total_fn = total_tn = 0.0\n",
    "    all_probs, all_targets = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        x = normalize_batch(x, MEAN_13, STD_13)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
    "            out = model(x)\n",
    "            logits = out.logits\n",
    "            logits = F.interpolate(\n",
    "                logits,\n",
    "                size=y.shape[-2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss_sum += loss.item() * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= thr).float()\n",
    "\n",
    "        total_tp += (preds * y).sum().item()\n",
    "        total_fp += (preds * (1 - y)).sum().item()\n",
    "        total_fn += ((1 - preds) * y).sum().item()\n",
    "        total_tn += ((1 - preds) * (1 - y)).sum().item()\n",
    "\n",
    "        all_probs.append(probs.flatten().detach().cpu())\n",
    "        all_targets.append(y.flatten().detach().cpu())\n",
    "\n",
    "    precision = (total_tp + eps) / (total_tp + total_fp + eps)\n",
    "    recall    = (total_tp + eps) / (total_tp + total_fn + eps)\n",
    "    f1        = (2 * precision * recall) / (precision + recall + eps)\n",
    "    acc       = (total_tp + total_tn + eps) / (\n",
    "        total_tp + total_tn + total_fp + total_fn + eps\n",
    "    )\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    auc = average_precision_score(all_targets, all_probs)\n",
    "\n",
    "    loss_global = total_loss_sum / total_samples\n",
    "\n",
    "    return loss_global, precision, recall, f1, acc, auc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, loader, thr=0.5, eps=1e-6):\n",
    "    model.eval()\n",
    "    total_loss_sum = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    total_tp = total_fp = total_fn = total_tn = 0.0\n",
    "    all_probs, all_targets = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        x = normalize_batch(x, MEAN_13, STD_13)\n",
    "\n",
    "        out = model(x)\n",
    "        logits = out.logits\n",
    "        logits = F.interpolate(\n",
    "                logits,\n",
    "                size=y.shape[-2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss_sum += loss.item() * bs\n",
    "        total_samples += bs\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= thr).float()\n",
    "\n",
    "        total_tp += (preds * y).sum().item()\n",
    "        total_fp += (preds * (1 - y)).sum().item()\n",
    "        total_fn += ((1 - preds) * y).sum().item()\n",
    "        total_tn += ((1 - preds) * (1 - y)).sum().item()\n",
    "\n",
    "        all_probs.append(probs.flatten().cpu())\n",
    "        all_targets.append(y.flatten().cpu())\n",
    "\n",
    "    precision = (total_tp + eps) / (total_tp + total_fp + eps)\n",
    "    recall    = (total_tp + eps) / (total_tp + total_fn + eps)\n",
    "    f1        = (2 * precision * recall) / (precision + recall + eps)\n",
    "    acc       = (total_tp + total_tn + eps) / (\n",
    "        total_tp + total_tn + total_fp + total_fn + eps\n",
    "    )\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    auc = average_precision_score(all_targets, all_probs)\n",
    "\n",
    "    loss_global = total_loss_sum / total_samples\n",
    "\n",
    "    return loss_global, precision, recall, f1, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T12:57:35.375158Z",
     "iopub.status.busy": "2026-01-04T12:57:35.374865Z",
     "iopub.status.idle": "2026-01-04T13:18:02.519150Z",
     "shell.execute_reply": "2026-01-04T13:18:02.518078Z",
     "shell.execute_reply.started": "2026-01-04T12:57:35.375131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unet_basic] Epoch 001 | tr_loss=0.2926 tr_f1=0.0010 tr_acc=0.9382 tr_PRauc=0.0551 || va_loss=0.2822 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.1213 | 187.0s\n",
      "[unet_basic] Epoch 002 | tr_loss=0.2200 tr_f1=0.0000 tr_acc=0.9411 tr_PRauc=0.0902 || va_loss=0.1836 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.1023 | 179.6s\n",
      "[unet_basic] Epoch 003 | tr_loss=0.2242 tr_f1=0.0000 tr_acc=0.9411 tr_PRauc=0.0789 || va_loss=0.2130 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.1231 | 180.2s\n",
      "[unet_basic] Epoch 004 | tr_loss=0.2260 tr_f1=0.0000 tr_acc=0.9411 tr_PRauc=0.0791 || va_loss=0.2155 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.0573 | 181.5s\n",
      "[unet_basic] Epoch 005 | tr_loss=0.2191 tr_f1=0.0000 tr_acc=0.9411 tr_PRauc=0.0905 || va_loss=0.1837 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.0800 | 179.1s\n",
      "[unet_basic] Epoch 006 | tr_loss=0.2090 tr_f1=0.0000 tr_acc=0.9411 tr_PRauc=0.1330 || va_loss=0.1727 va_f1=0.0000 va_acc=0.9522 va_PRauc=0.0906 | 177.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/4022586548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/437905945.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, thr, eps)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mall_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mloss_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0m_binary_uninterpolated_average_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0;31m     return _average_binary_score(\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0maverage_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_uninterpolated_average_precision\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0;31m         precision, recall, _ = precision_recall_curve(\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate, probas_pred)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobas_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m   1006\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;31m# sort scores and corresponding truth values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m     \u001b[0mdesc_score_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mergesort\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order, stable)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \"\"\"\n\u001b[0;32m-> 1200\u001b[0;31m     return _wrapfunc(\n\u001b[0m\u001b[1;32m   1201\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BEST_PATH = \"/kaggle/working/seg-b1.pth\"\n",
    "best_val_f1 = -1\n",
    "\n",
    "import pandas as pd\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, cfg.EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_prec, tr_rec, tr_f1, tr_acc, tr_auc = train_one_epoch(model, train_loader, thr=0.5)\n",
    "    va_loss, va_prec, va_rec, va_f1, va_acc, va_auc = validate_epoch(model, val_loader, thr=0.5)\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(\n",
    "        f\"[unet_basic] Epoch {epoch:03d} | \"\n",
    "        f\"tr_loss={tr_loss:.4f} tr_f1={tr_f1:.4f} tr_acc={tr_acc:.4f} tr_PRauc={tr_auc:.4f} || \"\n",
    "        f\"va_loss={va_loss:.4f} va_f1={va_f1:.4f} va_acc={va_acc:.4f} va_PRauc={va_auc:.4f} | \"\n",
    "        f\"{elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "    \n",
    "        \"tr_loss\": tr_loss,\n",
    "        \"tr_precision\": tr_prec,\n",
    "        \"tr_recall\": tr_rec,\n",
    "        \"tr_f1\": tr_f1,\n",
    "        \"tr_acc\": tr_acc,\n",
    "        \"tr_PRauc\": tr_auc,\n",
    "    \n",
    "        \"va_loss\": va_loss,\n",
    "        \"va_precision\": va_prec,\n",
    "        \"va_recall\": va_rec,\n",
    "        \"va_f1\": va_f1,\n",
    "        \"va_acc\": va_acc,\n",
    "        \"va_PRauc\": va_auc,\n",
    "    \n",
    "        \"seconds\": elapsed\n",
    "    })\n",
    "\n",
    "    if va_f1 > best_val_f1:\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"mean_13\": MEAN_13,\n",
    "            \"std_13\": STD_13,\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_f1\": best_val_f1,\n",
    "            \"va_auc\": va_auc,\n",
    "        }, BEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayangin: gambar mostly background, api cuma 1-2 pixel. Model prediksi 0 api → dice pixel-wise masih lumayan karena background cocok (misal 90% pixel benar). Tapi F1 object-wise → 0 karena dia miss all actual fire objects.\n",
    "\n",
    "- Liat epoch 14–19: F1 mulai naik (0.38, 0.35, 0.26). Itu karena model mulai ngerasa beberapa fire patch. Dice kadang turun dikit (0.64) karena beberapa prediksi terlalu kecil/lebih besar → overlap ga maksimal, tapi F1 object-wise meningkat karena ada setidaknya beberapa detection bener.\n",
    "\n",
    "- Misal ground truth fire cuma 10 pixel di gambar.\n",
    "Model prediksi fire 10 pixel, tepat di lokasi yang sama → overlap 100% → dice tinggi, F1 juga tinggi.\n",
    "Model prediksi fire 10 pixel tapi di tempat salah → overlap 0% → dice rendah, F1 rendah.\n",
    "Model prediksi 20 pixel, 10 di tempat benar, 10 di tempat salah → overlap 50% → dice turun, F1 juga turun karena false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history)\n",
    "\n",
    "df_history = df_history.round(4)\n",
    "csv_path = \"/kaggle/working/training_metrics_trans.csv\"\n",
    "df_history.to_csv(csv_path, index=False)\n",
    "print(\"Saved to:\", csv_path)\n",
    "df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_prec, test_rec, test_f1, test_acc, test_auc = validate_epoch(\n",
    "    model, test_loader\n",
    ")\n",
    "\n",
    "print(f\"Test Loss   : {test_loss:.4f}\")\n",
    "print(f\"Test Prec   : {test_prec:.4f}\")\n",
    "print(f\"Test Recall : {test_rec:.4f}\")\n",
    "print(f\"Test F1     : {test_f1:.4f}\")\n",
    "print(f\"Test Acc    : {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sen2FireDataset(Dataset):\n",
    "    def __init__(self, paths, with_label=True):\n",
    "        self.paths = paths\n",
    "        self.with_label = with_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        patch_name = os.path.basename(p)\n",
    "\n",
    "        with np.load(p) as d:\n",
    "            img12 = d[cfg.X_KEY].astype(np.float32)\n",
    "            aer   = d[cfg.A_KEY].astype(np.float32)[None, ...]\n",
    "            x = np.concatenate([img12, aer], axis=0)\n",
    "\n",
    "            if self.with_label:\n",
    "                y = d[cfg.Y_KEY]\n",
    "                if y.ndim == 2:\n",
    "                    y = y[None, ...]\n",
    "                y = (y > 0).astype(np.float32)\n",
    "\n",
    "                return torch.from_numpy(x), torch.from_numpy(y), patch_name\n",
    "            else:\n",
    "                return torch.from_numpy(x), patch_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(Sen2FireDataset(test_paths, with_label=True),\n",
    "                         batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=cfg.NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_selected_patches(loader, selected_paths):\n",
    "    # kumpulin semua sample dulu\n",
    "    all_samples = {}\n",
    "    for x, y, patch_name in loader:\n",
    "        for i, p in enumerate(patch_name):\n",
    "            all_samples[p] = {\n",
    "                \"img\": x[i].cpu(),\n",
    "                \"mask_gt\": y[i,0].cpu(),\n",
    "            }\n",
    "\n",
    "    fig, axes = plt.subplots(len(selected_paths), 3, figsize=(15, 5*len(selected_paths)))\n",
    "\n",
    "    if len(selected_paths) == 1:\n",
    "        axes = axes[None, :]  # konsisten\n",
    "\n",
    "    for i, path in enumerate(selected_paths):\n",
    "        sample = all_samples[path]\n",
    "\n",
    "        img = sample[\"img\"].permute(1,2,0).numpy()[..., :3]\n",
    "        img_vis = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "        mask_gt = sample[\"mask_gt\"].numpy()\n",
    "        # pred bisa pake model langsung\n",
    "        x_tensor = sample[\"img\"].unsqueeze(0).to(DEVICE)\n",
    "        x_tensor = normalize_batch(x_tensor, MEAN_13, STD_13)\n",
    "        with torch.no_grad():\n",
    "            mask_pred = (torch.sigmoid(model(x_tensor)) >= t_best)[0,0].cpu().numpy()\n",
    "\n",
    "        # Original\n",
    "        axes[i,0].imshow(img_vis)\n",
    "        axes[i,0].set_title(f\"{path}\\nOriginal\")\n",
    "        axes[i,0].axis('off')\n",
    "\n",
    "        # GT\n",
    "        axes[i,1].imshow(mask_gt, cmap='Reds')\n",
    "        axes[i,1].set_title(\"Ground Truth\")\n",
    "        axes[i,1].axis('off')\n",
    "\n",
    "        # Pred overlay\n",
    "        axes[i,2].imshow(img_vis)\n",
    "        axes[i,2].imshow(mask_pred, cmap='Reds', alpha=0.4)\n",
    "        axes[i,2].set_title(\"Prediction\")\n",
    "        axes[i,2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_paths = [\n",
    "    \"scene_3_patch_4_11.npz\",\n",
    "    \"scene_3_patch_5_35.npz\",\n",
    "    \"scene_4_patch_3_19.npz\",\n",
    "    \"scene_3_patch_5_12.npz\",\n",
    "    \"scene_4_patch_3_21.npz\",\n",
    "    \"scene_4_patch_18_13.npz\",\n",
    "    \"scene_4_patch_13_12.npz\",\n",
    "]\n",
    "\n",
    "show_selected_patches(test_loader, selected_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_paths = [\n",
    "    \"scene_4_patch_4_14.npz\",\n",
    "    \"scene_1_patch_20_9.npz\",\n",
    "    \"scene_3_patch_3_28.npz\",\n",
    "    \"scene_4_patch_2_14.npz\",\n",
    "    \"scene_4_patch_10_12.npz\",\n",
    "]\n",
    "\n",
    "show_selected_patches(test_loader, selected_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"scene_4_patch_31_17.npz\",\n",
    "    \"scene_1_patch_22_8.npz\",\n",
    "    \"scene_1_patch_21_10.npz\",\n",
    "    \"scene_3_patch_7_4.npz\",\n",
    "    \"scene_4_patch_13_23.npz\"\n",
    "]\n",
    "\n",
    "show_selected_patches(test_loader, paths)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8507641,
     "sourceId": 14342736,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 551707,
     "modelInstanceId": 538416,
     "sourceId": 708904,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
